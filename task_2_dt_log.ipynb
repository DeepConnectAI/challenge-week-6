{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Employee Attrition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "The dataset is available at <strong>\"data/attrition.csv\"</strong> in the respective challenge's repo.<br>\n",
    "This is a fictional data set created by IBM data scientists.\n",
    "\n",
    "#### Features (X)\n",
    "1. Age - Employee's current age. (Numeric)\n",
    "2. BusinessTravel - Frequency of travelling for business (Categorical)\n",
    "    - Travel Frequently\n",
    "    - Travel Rarely\n",
    "    - Not Travel at all\n",
    "3. DailyRate - Daily rate of earning. (Numeric)\n",
    "4. Department - Job-specific Department. (Categorical)\n",
    "    - R&D\n",
    "    - Sales\n",
    "    - HR\n",
    "5. DistanceFromHome (Numeric)\n",
    "6. Education (Numeric) -\n",
    "    - Below College (1)\n",
    "    - College (2)\n",
    "    - Bachelor (3)\n",
    "    - Master (4)\n",
    "    - Doctor (5)\n",
    "7. Education Field (Categorical) -\n",
    "    - Life Sciences\n",
    "    - Medical\n",
    "    - Marketing\n",
    "    - Techincal Degree\n",
    "    - Human Resources\n",
    "    - Other\n",
    "8. EmployeeCount (Numeric)\n",
    "9. EmployeeNumber (Numeric)\n",
    "10. EnvironmentSatisfaction (1-4) (Numeric)\n",
    "11. Gender (Binary)\n",
    "12. HourlyRate (Numeric)\n",
    "13. JobInvolvement (1-4) (Numeric)\n",
    "14. JobLevel (1-5) (Numeric)\n",
    "15. JobRole (Categorical)\n",
    "    - Research Scientist\n",
    "    - Laboratory Technician\n",
    "    - Manufacturing Director\n",
    "    - Healthcare Representative\n",
    "    - Manager\n",
    "    - Research Director\n",
    "    - Sales Executive\n",
    "    - Sales Representative\n",
    "    - Human Resources\n",
    "16. JobSatisfaction (1-4) (Numeric)\n",
    "17. MaritalStatus (Categorical)\n",
    "    - Married\n",
    "    - Single\n",
    "    - Divorced\n",
    "18. MonthlyIncome (Numeric)\n",
    "19. MonthlyRate (Numeric)\n",
    "20. NumCompaniesWorked (Numeric)\n",
    "21. OverTime (Yes/No) (Categorical)\n",
    "22. PercentSalaryHike (Numeric)\n",
    "23. PerformanceRating (1-4) (Numeric)\n",
    "24. RelationshipSatisfaction (1-4) (Numeric)\n",
    "25. StandardHours (numeric)\n",
    "26. StockOptionLevel (0-3) (Numeric)\n",
    "27. TotalWorkingYears (Numeric)\n",
    "28. TrainingTimesLastYear (0-5) (Numeric)\n",
    "29. WorkLifeBalance (1-4)(Numeric)\n",
    "30. YearsAtCompany (Numeric)\n",
    "31. YearsInCurrentRole (Numeric)\n",
    "32. YearsSinceLastPromotion(Numeric)\n",
    "33. YearsWithCurrManager (Numeric)\n",
    "\n",
    "#### Target (y)\n",
    "- Attrition (Binary)\n",
    "\n",
    "#### Objective\n",
    "- To apply Logistic Regression and Decision Tree Algorithms on the given dataset and understand the concepts of Underfitting and Overfitting and ways to combat these problems. There are three sections -\n",
    "- Section 1 \n",
    "    - Visualization of Overfitting and Underfitting (with results on cross validation)\n",
    "- Section 2\n",
    "    - Regularization\n",
    "- Section 3\n",
    "    - Early Stopping\n",
    "    - Pruning (w.r.t Decision Trees)\n",
    "\n",
    "#### Tasks\n",
    "- Download and load the data (csv file)\n",
    "- Process the data according to guidelines given in the comments of the respective cells.\n",
    "- Split the dataset into 60% for training and rest 40% for testing (sklearn.model_selection.train_test_split function)\n",
    "- Initialize Logistic Regression and Decision Tree Models (With parameters given in the cell)\n",
    "- Train the models on the same dataset\n",
    "- Complete Section 1,2 and 3\n",
    "\n",
    "#### Further Fun (will not be evaluated)\n",
    "- Train model on different train-test splits such as 60-40, 50-50, 70-30, 80-20, 90-10, 95-5 etc. and observe the respective plots and results on both X_train and X_test\n",
    "- Shuffle training samples with different random seed values in the train_test_split function. Check the model error for the testing data for each setup.\n",
    "- Explore ways to deal with imbalanced dataset. Use different methods (such as eliminating outliers and such) to experiment with the given dataset.\n",
    "\n",
    "#### Helpful links\n",
    "- pd.get_dummies() and One Hot Encoding: https://queirozf.com/entries/one-hot-encoding-a-feature-on-a-pandas-dataframe-an-example\n",
    "- Feature Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "- Train-test splitting: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "- Differences between Logistic Regression and a Decision Tree: https://www.geeksforgeeks.org/ml-logistic-regression-v-s-decision-tree-classification/\n",
    "- When are Decision Trees better than Logistic Regression?: https://www.displayr.com/decision-trees-are-usually-better-than-logistic-regression\n",
    "- How to choose between Logistic Regression and Decision Trees given a dataset: https://datascience.stackexchange.com/questions/6048/should-i-use-a-decision-tree-or-logistic-regression-for-classification\n",
    "- Decision Tree Classifier by Sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- Understanding classification metrics like Precision, Recall, F-Scores and Confusion matrices: https://nillsf.com/index.php/2020/05/23/confusion-matrix-accuracy-recall-precision-false-positive-rate-and-f-scores-explained/\n",
    "- Understanding the ROC Curve and the ROC-AUC: https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier, RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data.learning_plot import plot_learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git clone the repo \n",
    "!git clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from local cloud directory\n",
    "data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataframe rows just to see some samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print info about dataset\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Missing Values (if any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode Categorical Columns (if required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize/Standardize numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (60/40)\n",
    "X_train, X_test, y_train, y_test = train_test_split(?, ?, test_size=?, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be introducing you to a practical visualization of the concepts of Overfitting and Underfitting alongside Cross Validation as a metric to measure performance of your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the accuracies we get when the models overfit (LR and DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the models \n",
    "lr=LogisticRegression(class_weight='balanced',penalty='none',fit_intercept=False)\n",
    "dt=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the models\n",
    "lr.fit(?,?)\n",
    "dt.fit(?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the accuracies of the training and test splits for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------LOGISTIC REGRESSION----------\")\n",
    "print(\"Accuracy of Training Split :\",accuracy_score(?,?))\n",
    "print(\"Accuracy of Test Split :\",accuracy_score(?,?))\n",
    "print()\n",
    "print(\"----------DECISION TREE----------\")\n",
    "print(\"Accuracy of Training Split :\",accuracy_score(?,?))\n",
    "print(\"Accuracy of Test Split :\",accuracy_score(?,?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see above, even though both models perform very well on the training set, they fail to show the same promise for the testing result. Now let's visualize our findings.\n",
    "\n",
    "<strong>Note</strong> - For cross validation, one can also use KFolds or StratifiedKFolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are splitting the original X into 10 train/test splits and\n",
    "reinitializing our models and thereby applying them onto the splits for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation splitting for evaluation of Logistic Regression\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then feed in X, y and the splits into our plot_learning_curve function which is user-defined and already imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Estimator\n",
    "estimator = LogisticRegression(class_weight='balanced',penalty='none')\n",
    "\n",
    "#Plot of Learning Curve (over original X and then cross val)\n",
    "plt.style.use(\"seaborn\")\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "plot_learning_curve(estimator, title, ?, ?, axes=axes, ylim=(0.7, 1.01),\n",
    "                    cv=?, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation splitting for evaluation of Decision Tree\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Estimator\n",
    "estimator = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of Learning Curve (over original X and then cross val)\n",
    "title = r\"Learning Curves (Decision Tree)\"\n",
    "plt.style.use(\"seaborn\")\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "plot_learning_curve(estimator, title, ?, ?, axes=axes, ylim=(0.7, 1.01),\n",
    "                    cv=?, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the accuracies we get when the models underfit (LR and DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the models\n",
    "lr=LogisticRegression(fit_intercept=False,class_weight='balanced',C=0.001)\n",
    "dt=DecisionTreeClassifier(max_leaf_nodes=2,max_features='log2',class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the models\n",
    "lr.fit(?,?)\n",
    "dt.fit(?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the accuracies of the training and test splits for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------LOGISTIC REGRESSION----------\")\n",
    "print(\"Accuracy of Training Split :\",accuracy_score(?,?))\n",
    "print(\"Accuracy of Test Split :\",accuracy_score(?,?))\n",
    "print()\n",
    "print(\"----------DECISION TREE----------\")\n",
    "print(\"Accuracy of Training Split :\",accuracy_score(?,?))\n",
    "print(\"Accuracy of Test Split :\",accuracy_score(?,?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see above, both models perform poorly as far as training is concerned. As a result, we also perform poorly on the testing set. This happened because of the hyperparameters set in a specific way. Now, let's visualize this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation splitting for evaluation of Logistic Regression\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Estimator\n",
    "estimator = LogisticRegression(fit_intercept=False,class_weight='balanced',C=0.001)\n",
    "\n",
    "#Plot of Learning Curve (over original X and then cross val)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves (Logistic Regression)\"\n",
    "plot_learning_curve(estimator, title, ?, ?, axes=axes, ylim=(0.1, 1.01),\n",
    "                    cv=?, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation splitting for evaluation of Decision Tree\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Estimator\n",
    "estimator = DecisionTreeClassifier(max_leaf_nodes=2,max_features='log2',class_weight='balanced')\n",
    "\n",
    "#Plot of Learning Curve (over original X and then cross val)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "title = \"Learning Curves (Decision Tree)\"\n",
    "plot_learning_curve(estimator, title, ?, ?, axes=axes, ylim=(0.1, 1.01),\n",
    "                    cv=?, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha: Regularization Strength, Larger values specify stronger regularization\n",
    "alphas = np.logspace(10, -3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Ridge CLassifier on different values of alpha\n",
    "ridge_coefs = []\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "for a in alphas:\n",
    "    ridge = RidgeClassifier(alpha = a, fit_intercept = True, normalize = True)\n",
    "    ridge.fit(?,?)\n",
    "    train_losses.append(log_loss(?, ridge._predict_proba_lr(?)))\n",
    "    test_losses.append(log_loss(?, ridge._predict_proba_lr(?)))\n",
    "    ridge_coefs.append(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make ridge_coefs numpy array of shape (no_of_alphas,no_of_features)\n",
    "ridge_coefs ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing how coefficients vary with value of alpha\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, ridge_coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of Lambda')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Train Loss v/s Values of alpha\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "plt.plot(alphas, train_losses)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of Lambda')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Test Loss v/s Values of alpha\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "plt.plot(alphas, test_losses)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of Lambda')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c : Inverse of regularization strength; smaller values specify stronger regularization.\n",
    "C= np.logspace(-10,3,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training logisitic Regression with l1 penalty for different values of C\n",
    "lasso_coefs = []\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "for c in C:\n",
    "    lr = LogisticRegression(penalty='l1', C=c, fit_intercept=True, solver='liblinear')\n",
    "    lr.fit(?,?)\n",
    "    train_losses.append(log_loss(?, lr.predict_proba(?)))\n",
    "    test_losses.append(log_loss(?,lr.predict_proba(?)))\n",
    "    lasso_coefs.append(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make lasso_coefs numpy array of shape (no_of_C,no_of_features)\n",
    "lasso_coefs="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot showing how coefficients vary with value of c\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "ax.plot(C, lasso_coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of C')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Train Loss v/s Values of C\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "plt.plot(C, train_losses)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of C')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Train Loss v/s Values of C\n",
    "plt.style.use(\"seaborn\")\n",
    "ax = plt.gca()\n",
    "plt.plot(C, test_losses)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Value of C')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(?,?, test_size=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the model with given parameters\n",
    "estimator = LogisticRegression(class_weight='balanced',penalty='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to training data\n",
    "estimator.fit(?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict train values\n",
    "y_train_pred= estimator.predict(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test values\n",
    "y_test_pred= estimator.predict(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train accuracy\n",
    "accuracy_score(?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test accuracy\n",
    "accuracy_score(?,?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train score > Test Score, which indicates overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will try to see how stopping at the right the no. of iterations will help us increase test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc=[]\n",
    "train_acc=[]\n",
    "iterr=[]\n",
    "\n",
    "# We will take a range of max_iteratations from 10 to 300\n",
    "for i in range(10,300):\n",
    "    \n",
    "    #Initializing the model with given parameters and max_iter as i\n",
    "    estimator = LogisticRegression(class_weight='balanced',penalty='none',max_iter = ?)\n",
    "    \n",
    "    #fit the model to training data\n",
    "    estimator.fit(?,?)\n",
    "    \n",
    "    # Predict train values\n",
    "    y_train_pred= estimator.predict(?)\n",
    "    \n",
    "    # Predict test values\n",
    "    y_test_pred= estimator.predict(?)\n",
    "    \n",
    "    #append train accuracy to list\n",
    "    train_acc.append(accuracy_score(?,?))\n",
    "    \n",
    "    #append test accuracy to list\n",
    "    test_acc.append(accuracy_score(?,?))\n",
    "    \n",
    "    # append the value of max_iter to list\n",
    "    iterr.append(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Training and testing accuracies against the number of iterations\n",
    "plt.xlabel(\"max_iter\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(iterr,?,label='train_accuracy')\n",
    "plt.plot(iterr,?,label='test_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above plot shows us that we need to stop after a certain number of iterations to get the optimal value of test accurcy \n",
    "### The sweet spot is where we strike a balance between bias and variance (aka the bias-variance trade-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning in Decision Trees\n",
    "### Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict train values\n",
    "y_train_pred= dt.predict(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test values\n",
    "y_test_pred= dt.predict(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train score\n",
    "accuracy_score(?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test score\n",
    "accuracy_score(?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_tree=[]\n",
    "train_acc_tree=[]\n",
    "max_dep=[]\n",
    "for i in range(1,50):\n",
    "    #Initializing the model with given parameters and max_depth as i\n",
    "    estimator = DecisionTreeClassifier(class_weight='balanced',max_depth=i,criterion='entropy')\n",
    "    \n",
    "    #fit the model to training data\n",
    "    estimator.fit(?,?)\n",
    "    \n",
    "    # Predict train values\n",
    "    y_train_pred= estimator.predict(?)\n",
    "    \n",
    "    # Predict test values\n",
    "    y_test_pred= estimator.predict(?)\n",
    "    \n",
    "    #append train accuracy to list\n",
    "    train_acc_tree.append(accuracy_score(?,?))\n",
    "    \n",
    "    #append test accuracy to list\n",
    "    test_acc_tree.append(accuracy_score(?,?))\n",
    "    \n",
    "    #append value of max_depth to list\n",
    "    max_dep.append(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot training and testing accuracies against varying values of max depth\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(max_dep,?,label='train_accuracy')\n",
    "plt.plot(max_dep,?,label='test_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we see that after a certain value of max depth, the train accuracy stops increasing. We take that as the optimal value of max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
